{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learning\n",
    "- Algorithms from ```pp. 110 - 111``` in Sutton & Barto 2017\n",
    "- Double Q-learning algorithm employs to action-value functions (i.e., $Q_1 and Q_2$) to avoid maximization bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn, random\n",
    "\n",
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.normal()\n",
    "    for action in env.actions:\n",
    "        q[0, action] = 0\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'D'): 0,\n",
       " (0, 'L'): 0,\n",
       " (0, 'R'): 0,\n",
       " (0, 'U'): 0,\n",
       " (1, 'D'): 1.0159263422385194,\n",
       " (1, 'L'): -0.9015277224413166,\n",
       " (1, 'R'): -0.8401929147381398,\n",
       " (1, 'U'): -0.3866748964867951,\n",
       " (2, 'D'): -1.6171831004555488,\n",
       " (2, 'L'): -0.757413177831847,\n",
       " (2, 'R'): 1.1948020656778442,\n",
       " (2, 'U'): -0.6814167904466197,\n",
       " (3, 'D'): 0.8048046240684537,\n",
       " (3, 'L'): 0.5058075927359411,\n",
       " (3, 'R'): -0.2396998941031068,\n",
       " (3, 'U'): 0.2155229655857796,\n",
       " (4, 'D'): -1.6147931728590075,\n",
       " (4, 'L'): -0.7788455232241043,\n",
       " (4, 'R'): -1.4834265811813951,\n",
       " (4, 'U'): -0.6831094436413707,\n",
       " (5, 'D'): 0.553787569878218,\n",
       " (5, 'L'): -0.7535460982740707,\n",
       " (5, 'R'): -0.5446399864366045,\n",
       " (5, 'U'): 0.3275946472542217,\n",
       " (6, 'D'): 0.4967878126487371,\n",
       " (6, 'L'): -0.08765553689024165,\n",
       " (6, 'R'): 2.561671991266108,\n",
       " (6, 'U'): -0.6251803976764682,\n",
       " (7, 'D'): -1.4239033515729715,\n",
       " (7, 'L'): 0.8961390571157126,\n",
       " (7, 'R'): -1.752510423129062,\n",
       " (7, 'U'): 0.186255694272231,\n",
       " (8, 'D'): 2.0464864623182057,\n",
       " (8, 'L'): -1.4145441882793333,\n",
       " (8, 'R'): -0.01479790948318322,\n",
       " (8, 'U'): -0.5827888737714622,\n",
       " (9, 'D'): -1.99334821662137,\n",
       " (9, 'L'): -0.2143702145160287,\n",
       " (9, 'R'): 0.3093816164828963,\n",
       " (9, 'U'): -0.13810316479560683,\n",
       " (10, 'D'): -0.4286970274384264,\n",
       " (10, 'L'): -1.0313201895326287,\n",
       " (10, 'R'): 0.674733204171753,\n",
       " (10, 'U'): -0.06485579239030625,\n",
       " (11, 'D'): -1.08761491406436,\n",
       " (11, 'L'): 0.37568985747762257,\n",
       " (11, 'R'): 0.6549082083274831,\n",
       " (11, 'U'): -0.044872797471142374,\n",
       " (12, 'D'): -0.8474472601122482,\n",
       " (12, 'L'): -0.6624032631781939,\n",
       " (12, 'R'): 0.4720691318622512,\n",
       " (12, 'U'): -1.2333806138625314,\n",
       " (13, 'D'): -0.38599325593593187,\n",
       " (13, 'L'): -1.677564721893633,\n",
       " (13, 'R'): -1.1425774085316547,\n",
       " (13, 'U'): -1.3606173055898687,\n",
       " (14, 'D'): -1.1641372208339686,\n",
       " (14, 'L'): 1.1457975665987994,\n",
       " (14, 'R'): -1.012618770180745,\n",
       " (14, 'U'): -0.06091464445082383}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_value(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_greedy_policy(env, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        q_values = []\n",
    "        prob = []\n",
    "        \n",
    "        for a in env.actions:\n",
    "            actions.append(a)\n",
    "            q_values.append(Q[state,a])   \n",
    "        for i in range(len(q_values)):\n",
    "            if i == np.argmax(q_values):\n",
    "                prob.append(1)\n",
    "            else:\n",
    "                prob.append(0)       \n",
    "                \n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy(env, e, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    prob = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[(state, action)])\n",
    "    for i in range(len(action_values)):\n",
    "        if i == np.argmax(action_values):\n",
    "            prob.append(1 - e + e/len(action_values))\n",
    "        else:\n",
    "            prob.append(e/len(action_values))\n",
    "    return np.random.choice(actions, p = prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy(env, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[state, action])\n",
    "    return actions[np.argmax(action_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def double_q_learning(env, epsilon, alpha, num_iter):\n",
    "    Q1, Q2  = state_action_value(env), state_action_value(env)\n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        while current_state != 0:\n",
    "            Q = dict()\n",
    "            for key in Q1.keys():\n",
    "                Q[key] = Q1[key] + Q2[key]\n",
    "            current_action = e_greedy(env, epsilon, Q, current_state)\n",
    "            next_state, reward = env.state_transition(current_state, current_action)\n",
    "            \n",
    "            # choose Q1 or Q2 with equal probabilities (0.5)\n",
    "            chosen_Q = np.random.choice([\"Q1\", \"Q2\"])\n",
    "            if chosen_Q == \"Q1\":    # when Q1 is chosen\n",
    "                best_action = greedy(env, Q1, next_state)\n",
    "                Q1[current_state, current_action] += alpha * \\\n",
    "                    (reward + env.gamma * Q2[next_state, best_action] - Q1[current_state, current_action])\n",
    "            else:                    # when Q2 is chosen\n",
    "                best_action = greedy(env, Q2, next_state)\n",
    "                Q2[current_state, current_action] += alpha * \\\n",
    "                    (reward + env.gamma * Q1[next_state, best_action] - Q2[current_state, current_action])\n",
    "                    \n",
    "            current_state = next_state\n",
    "    return Q1, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q1, Q2 = double_q_learning(gw, 0.2, 0.5, 5000)\n",
    "\n",
    "# sum Q1 & Q2 elementwise to obtain final Q-values\n",
    "Q = dict()\n",
    "for key in Q1.keys():\n",
    "    Q[key] = Q1[key] + Q2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi_hat = generate_greedy_policy(gw, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi_hat[s][0][np.argmax(pi_hat[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1da1fa52390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi_hat, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
