{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$-step off-policy learning without importance sampling\n",
    "- Algorithms from ```pp. 124 - 125``` in Sutton & Barto 2017\n",
    "    - $n$-step Tree Backup Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn, random\n",
    "\n",
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.normal()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy(env, e, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    prob = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[(state, action)])\n",
    "    for i in range(len(action_values)):\n",
    "        if i == np.argmax(action_values):\n",
    "            prob.append(1 - e + e/len(action_values))\n",
    "        else:\n",
    "            prob.append(e/len(action_values))\n",
    "    return actions, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_e_greedy_policy(env, e, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        pi[state] = e_greedy(env, e, Q, state)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for tree backup algorithm\n",
    "def avg_over_actions(pi, Q, state):\n",
    "    actions, probs = pi[state]\n",
    "    q_values = np.zeros(4)\n",
    "    for s, a in Q.keys():\n",
    "        if s == state:\n",
    "            q_values[actions.index(a)] = Q[s,a]\n",
    "    return np.dot(q_values, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $n$-step off-policy learning without importance sampling\n",
    "- The target includes also the estimated values of dangling action nodes hanging off the sides, at all levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_step_tree_backup(env, epsilon, alpha, n, num_iter, learn_pi = True):\n",
    "    Q = state_action_value(env)\n",
    "    Q_, pi_, delta = dict(), dict(), dict()  \n",
    "    pi = generate_e_greedy_policy(env, epsilon, Q) \n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        action = np.random.choice(b[current_state][0], p = b[current_state][1])\n",
    "        state_trace, action_trace, reward_trace  = [current_state], [action], [0]\n",
    "        Q_[0] = Q[current_state, action]\n",
    "        t, T = 0, 10000\n",
    "        while True:\n",
    "            if t < T:    \n",
    "                next_state, reward = env.state_transition(current_state, action)\n",
    "                state_trace.append(next_state)\n",
    "                reward_trace.append(reward)\n",
    "                if next_state == 0:\n",
    "                    T = t + 1\n",
    "                    delta[t] = reward - Q_[t]\n",
    "                else:  \n",
    "                    delta[t] = reward + env.gamma * avg_over_actions(pi, Q, next_state) - Q_[t]\n",
    "                    action = np.random.choice(pi[next_state][0], p = pi[next_state][1])\n",
    "                    action_trace.append(action)\n",
    "                    Q_[t+1] = Q[next_state, action]\n",
    "                    pi_[t+1] = pi[next_state][1][pi[next_state][0].index(action)]\n",
    "                    \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                Z = 1\n",
    "                G = Q_[tau]\n",
    "                for i in range(tau, min([tau + n -1, T-1])):\n",
    "                    G += Z * delta[i]\n",
    "                    Z *= env.gamma * pi_[i+1]\n",
    "                Q[state_trace[tau], action_trace[tau]] += alpha * (G - Q[state_trace[tau], action_trace[tau]])\n",
    "                if learn_pi:\n",
    "                    pi[state_trace[tau]] = e_greedy(env, epsilon, Q, state_trace[tau])\n",
    "            current_state = next_state    \n",
    "#             print(state_trace, action_trace, reward_trace)\n",
    "            \n",
    "            if tau == (T-1):\n",
    "                break\n",
    "            t += 1\n",
    "            \n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi, Q = n_step_tree_backup(gw, 0.2, 0.5, 1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZKWtiwAl/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f5bab06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
