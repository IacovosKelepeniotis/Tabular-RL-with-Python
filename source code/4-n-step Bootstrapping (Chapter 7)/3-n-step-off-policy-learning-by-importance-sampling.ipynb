{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$-step off-policy learning by importance sampling\n",
    "- Algorithms from ```pp. 121 - 122``` in Sutton & Barto 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn, random\n",
    "\n",
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.normal()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy(env, e, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    prob = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[(state, action)])\n",
    "    for i in range(len(action_values)):\n",
    "        if i == np.argmax(action_values):\n",
    "            prob.append(1 - e + e/len(action_values))\n",
    "        else:\n",
    "            prob.append(e/len(action_values))\n",
    "    return actions, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_e_greedy_policy(env, e, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        pi[state] = e_greedy(env, e, Q, state)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $n$-step off-policy learning\n",
    "- Calculates importance sampling ratio, by taking advantage of two policies (```pi``` and ```b```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_step_off_policy_learning(env, b, epsilon, alpha, n, num_iter, learn_pi = True):\n",
    "    Q = state_action_value(env)\n",
    "    pi = generate_e_greedy_policy(env, epsilon, Q) \n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        action = np.random.choice(b[current_state][0], p = b[current_state][1])\n",
    "        state_trace, action_trace, reward_trace  = [current_state], [action], [0]\n",
    "        t, T = 0, 10000\n",
    "        while True:\n",
    "            if t < T:    \n",
    "                next_state, reward = env.state_transition(current_state, action)\n",
    "                state_trace.append(next_state)\n",
    "                reward_trace.append(reward)\n",
    "                if next_state == 0:\n",
    "                    T = t + 1\n",
    "                else:  \n",
    "                    action = np.random.choice(b[next_state][0], p = b[next_state][1])\n",
    "                    action_trace.append(action)\n",
    "                    \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                rho = 1\n",
    "                for i in range(tau + 1, min([tau + n - 1, T - 1])):\n",
    "                    y = pi[state_trace[i]][1][pi[state_trace[i]][0].index(action_trace[i])]\n",
    "                    x = b[state_trace[i]][1][b[state_trace[i]][0].index(action_trace[i])]\n",
    "                    rho *= y / x\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min([tau + n, T]) + 1):\n",
    "                    G += (env.gamma ** (i - tau - 1)) * reward_trace[i-1]\n",
    "                if tau + n < T:\n",
    "                    G += (env.gamma ** n) * Q[state_trace[tau + n], action_trace[tau + n]]\n",
    "                Q[state_trace[tau], action_trace[tau]] += alpha * rho * (G - Q[state_trace[tau], action_trace[tau]])\n",
    "                if learn_pi:\n",
    "                    pi[state_trace[tau]] = e_greedy(env, epsilon, Q, state_trace[tau])\n",
    "            current_state = next_state    \n",
    "#             print(state_trace, action_trace, reward_trace)\n",
    "            \n",
    "            if tau == (T-1):\n",
    "                break\n",
    "            t += 1\n",
    "            \n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# any policy with random probabilities > 0\n",
    "b = generate_random_policy(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi, Q = n_step_off_policy_learning(gw, b, 0.2, 0.1, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (('U', 'D', 'L', 'R'), [0.05, 0.05, 0.8500000000000001, 0.05]),\n",
       " 2: (('U', 'D', 'L', 'R'), [0.05, 0.05, 0.8500000000000001, 0.05]),\n",
       " 3: (('U', 'D', 'L', 'R'), [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 4: (('U', 'D', 'L', 'R'), [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 5: (('U', 'D', 'L', 'R'), [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 6: (('U', 'D', 'L', 'R'), [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 7: (('U', 'D', 'L', 'R'), [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 8: (('U', 'D', 'L', 'R'), [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 9: (('U', 'D', 'L', 'R'), [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 10: (('U', 'D', 'L', 'R'), [0.05, 0.05, 0.05, 0.8500000000000001]),\n",
       " 11: (('U', 'D', 'L', 'R'), [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 12: (('U', 'D', 'L', 'R'), [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 13: (('U', 'D', 'L', 'R'), [0.05, 0.05, 0.05, 0.8500000000000001]),\n",
       " 14: (('U', 'D', 'L', 'R'), [0.05, 0.05, 0.05, 0.8500000000000001])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi[s][0][np.argmax(pi[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e46da7e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi, gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_over_actions(pi, Q, state):\n",
    "    actions, probs = pi[state]\n",
    "    q_values = np.zeros(4)\n",
    "    for s, a in Q.keys():\n",
    "        if s == state:\n",
    "            q_values[actions.index(a)] = Q[s,a]\n",
    "    return np.dot(q_values, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.025, 0.925]),\n",
       " 2: (('U', 'D', 'L', 'R'), [0.925, 0.025, 0.025, 0.025]),\n",
       " 3: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.025, 0.925]),\n",
       " 4: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.025, 0.925]),\n",
       " 5: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.925, 0.025]),\n",
       " 6: (('U', 'D', 'L', 'R'), [0.025, 0.925, 0.025, 0.025]),\n",
       " 7: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.925, 0.025]),\n",
       " 8: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.025, 0.925]),\n",
       " 9: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.925, 0.025]),\n",
       " 10: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.025, 0.925]),\n",
       " 11: (('U', 'D', 'L', 'R'), [0.025, 0.025, 0.025, 0.925]),\n",
       " 12: (('U', 'D', 'L', 'R'), [0.025, 0.925, 0.025, 0.025]),\n",
       " 13: (('U', 'D', 'L', 'R'), [0.925, 0.025, 0.025, 0.025]),\n",
       " 14: (('U', 'D', 'L', 'R'), [0.925, 0.025, 0.025, 0.025])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_e_greedy_policy(gw, 0.1, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_step_tree_backup(env, epsilon, alpha, n, num_iter, learn_pi = True):\n",
    "    Q = state_action_value(env)\n",
    "    Q_, pi_, delta = dict(), dict(), dict()  \n",
    "    pi = generate_e_greedy_policy(env, epsilon, Q) \n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        action = np.random.choice(b[current_state][0], p = b[current_state][1])\n",
    "        state_trace, action_trace, reward_trace  = [current_state], [action], [0]\n",
    "        Q_[0] = Q[current_state, action]\n",
    "        t, T = 0, 10000\n",
    "        while True:\n",
    "            if t < T:    \n",
    "                next_state, reward = env.state_transition(current_state, action)\n",
    "                state_trace.append(next_state)\n",
    "                reward_trace.append(reward)\n",
    "                if next_state == 0:\n",
    "                    T = t + 1\n",
    "                    delta[t] = reward - Q_[t]\n",
    "                else:  \n",
    "                    delta[t] = reward + env.gamma * avg_over_actions(pi, Q, next_state) - Q_[t]\n",
    "                    action = np.random.choice(pi[next_state][0], p = pi[next_state][1])\n",
    "                    action_trace.append(action)\n",
    "                    Q_[t+1] = Q[next_state, action]\n",
    "                    pi_[t+1] = pi[next_state][1][pi[next_state][0].index(action)]\n",
    "                    \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                Z = 1\n",
    "                G = Q_[tau]\n",
    "                for i in range(tau, min([tau + n -1, T-1])):\n",
    "                    G += Z * delta[i]\n",
    "                    Z *= env.gamma * pi_[i+1]\n",
    "                Q[state_trace[tau], action_trace[tau]] += alpha * (G - Q[state_trace[tau], action_trace[tau]])\n",
    "                if learn_pi:\n",
    "                    pi[state_trace[tau]] = e_greedy(env, epsilon, Q, state_trace[tau])\n",
    "            current_state = next_state    \n",
    "#             print(state_trace, action_trace, reward_trace)\n",
    "            \n",
    "            if tau == (T-1):\n",
    "                break\n",
    "            t += 1\n",
    "            \n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi, Q = n_step_tree_backup(gw, 0.2, 0.5, 1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZKWtiwAl/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f5bab06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
