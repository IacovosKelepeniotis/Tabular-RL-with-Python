{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Exploring Starts\n",
    "- Algorithms from ```pp. 76 - 78``` in Sutton & Barto 2017\n",
    "- Estimates optimal policy $\\pi \\approx \\pi_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn, random\n",
    "\n",
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate functions for generating policy\n",
    "- ```generate_random_policy()```: generates equiprobable random policy\n",
    "- ```generate_any_policy()```: generates any policy, assigning random probability for each action in each step\n",
    "- ```generate_greedy_policy()```: generates greedy policy (i.e., maximizer) based on action value function (```Q```) one currently has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_any_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        r = sorted(np.random.sample(3))\n",
    "        actions = env.actions\n",
    "        prob = [r[0], r[1] - r[0], r[2] - r[1], 1-r[2]]\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_greedy_policy(env, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        q_values = []\n",
    "        prob = []\n",
    "        \n",
    "        for a in env.actions:\n",
    "            actions.append(a)\n",
    "            q_values.append(Q[state,a])   \n",
    "        for i in range(len(q_values)):\n",
    "            if i == np.argmax(q_values):\n",
    "                prob.append(1)\n",
    "            else:\n",
    "                prob.append(0)       \n",
    "                \n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-action value\n",
    "- Function for generating $Q$ value, given state and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = - 1000        # arbitrarily negative value to avoid infinite loop\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 2: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 3: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 4: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 5: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 6: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 7: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 8: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 9: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 10: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 11: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 12: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 13: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25]),\n",
       " 14: (['U', 'D', 'L', 'R'], [0.25, 0.25, 0.25, 0.25])}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with random policy\n",
    "pi = generate_random_policy(gw)\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_episode(env, s0, a0, policy):\n",
    "    episode = []\n",
    "    done = False\n",
    "    current_state, action = s0, a0\n",
    "    episode.append((current_state, action, -1))\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward = gw.state_transition(current_state, action)\n",
    "        action = np.random.choice(policy[current_state][0], p = policy[current_state][1])\n",
    "        episode.append((next_state, action, reward))\n",
    "        \n",
    "        if next_state == 0 or len(episode) > 50:   # to prvent infinite loop due to greedy policy\n",
    "            done = True\n",
    "        current_state = next_state\n",
    "        \n",
    "    if len(episode) > 50:\n",
    "        return None\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_episode(env, s0, a0, policy):\n",
    "    episode = []\n",
    "    done = False\n",
    "    current_state, action = s0, a0\n",
    "    episode.append((current_state, action, -1))\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward = gw.state_transition(current_state, action)\n",
    "        pr = policy[current_state][1]\n",
    "        ## to make non-deterministic episode (mostly to avoid infinite episode due to greediness)\n",
    "        pr[np.argmax(pr)] -= .2\n",
    "        pr[np.random.choice(np.delete(np.arange(4), np.argmax(pr)))] += .1\n",
    "        pr[np.random.choice(np.delete(np.arange(4), np.argmax(pr)))] += .1\n",
    "        ##\n",
    "        action = np.random.choice(policy[current_state][0], p = pr)\n",
    "        episode.append((next_state, action, reward))\n",
    "        \n",
    "        if next_state == 0:   \n",
    "            done = True\n",
    "        current_state = next_state\n",
    "    return episode[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo ES (Exploring Starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first-visit MC\n",
    "def monte_carlo_es(env, num_iter):\n",
    "    Q = state_action_value(env)\n",
    "    pi = generate_any_policy(env)\n",
    "    returns = dict()\n",
    "    for s, a in Q.keys():\n",
    "        returns[(s,a)] = []\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        s0, a0 = np.random.choice(env.states), np.random.choice(env.actions)\n",
    "        episode = generate_episode(env, s0, a0, pi)\n",
    "        already_visited = set()\n",
    "        \n",
    "        for s, a, r in episode:\n",
    "            if (s, a) not in already_visited:\n",
    "                already_visited.add((s, a))\n",
    "                idx = episode.index((s, a, r))\n",
    "                G = 0\n",
    "                for j in range(idx, len(episode)-1):\n",
    "                    G = env.gamma * (G + episode[j][-1])\n",
    "                returns[(s,a)].append(G)\n",
    "                Q[(s,a)] = np.mean(returns[(s,a)])\n",
    "        pi = generate_greedy_policy(env, Q)\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.1 ms\n"
     ]
    }
   ],
   "source": [
    "# Obtained Estimates for Q & pi after 10 iterations\n",
    "Q_hat, pi_hat = monte_carlo_es(gw, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 'D'): -3.9167144623439669,\n",
       " (1, 'L'): -0.90000000000000024,\n",
       " (1, 'R'): -3.7996358633512988,\n",
       " (1, 'U'): -2.35066872435713,\n",
       " (2, 'D'): -5.3455230552791511,\n",
       " (2, 'L'): -2.5088924010670741,\n",
       " (2, 'R'): -5.4656931508050857,\n",
       " (2, 'U'): -3.9517965075225425,\n",
       " (3, 'D'): -3.7043205598016145,\n",
       " (3, 'L'): -5.4316464783665026,\n",
       " (3, 'R'): -4.6953086936176049,\n",
       " (3, 'U'): -4.7101393148904913,\n",
       " (4, 'D'): -3.8008933244757497,\n",
       " (4, 'L'): -2.3436989583598207,\n",
       " (4, 'R'): -3.9024374840192548,\n",
       " (4, 'U'): -0.90000000000000036,\n",
       " (5, 'D'): -5.2962197871005676,\n",
       " (5, 'L'): -2.814651384292373,\n",
       " (5, 'R'): -5.3308380552699255,\n",
       " (5, 'U'): -2.8146579950183321,\n",
       " (6, 'D'): -3.6729626177625252,\n",
       " (6, 'L'): -5.2486793781630414,\n",
       " (6, 'R'): -3.6738439414576596,\n",
       " (6, 'U'): -5.3155611578208282,\n",
       " (7, 'D'): -2.3741177208691879,\n",
       " (7, 'L'): -4.5578684949132038,\n",
       " (7, 'R'): -3.8934994687414548,\n",
       " (7, 'U'): -4.8399308139425683,\n",
       " (8, 'D'): -4.6992508422629458,\n",
       " (8, 'L'): -3.9803276623130852,\n",
       " (8, 'R'): -5.3200167723677456,\n",
       " (8, 'U'): -2.4510358295882031,\n",
       " (9, 'D'): -3.6532287653602555,\n",
       " (9, 'L'): -5.1917292411606573,\n",
       " (9, 'R'): -3.653017429049807,\n",
       " (9, 'U'): -5.2166701058940905,\n",
       " (10, 'D'): -2.6609939583436772,\n",
       " (10, 'L'): -4.5926525958950517,\n",
       " (10, 'R'): -2.6610251602837653,\n",
       " (10, 'U'): -4.5966881287642058,\n",
       " (11, 'D'): -0.90000000000000024,\n",
       " (11, 'L'): -3.7605564293210008,\n",
       " (11, 'R'): -2.3150245002265581,\n",
       " (11, 'U'): -3.7539872626923856,\n",
       " (12, 'D'): -4.7351631074157519,\n",
       " (12, 'L'): -4.6678422316264365,\n",
       " (12, 'R'): -4.7887317699039231,\n",
       " (12, 'U'): -3.7569123510943214,\n",
       " (13, 'D'): -3.8272573037511823,\n",
       " (13, 'L'): -5.2619338774014928,\n",
       " (13, 'R'): -2.3530617492516317,\n",
       " (13, 'U'): -4.5168995670386689,\n",
       " (14, 'D'): -2.2536423171174422,\n",
       " (14, 'L'): -3.6985339111824884,\n",
       " (14, 'R'): -0.90000000000000024,\n",
       " (14, 'U'): -3.7428348388087116}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi_hat[s][0][np.argmax(pi_hat[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACspJREFUeJzt3UtoXAXfx/F/mhEV4wUsSBGEbpoMPJDiwl0XgcZLQUKl\nYKtEQbJy0RRqkZh6wUtbV4KXULUrFz5xZXEhFGuLgoKLQguBiSKCeNu4krbYtM48C3n64kub1+Rl\nfmOmn88uZ8g5P3oo355DQgc6nU6nAICuWtfrAQBwLRBcAAgQXAAIEFwACBBcAAgQXAAIaHTz5M1m\ns1qLi928BF3SHBmpf7Xcu7Vqoen+rVULzZGqqmp96P6tRc3tI9Vqta74mSdcAAgQXAAIEFwACBBc\nAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwA\nCBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAI\nEFwACBBcAAj428Ftt9vd3AEAfa2x3Ic//PBDHTx4sBYWFqrRaFS73a5NmzbVzMxMbdy4MbURANa8\nZYM7Oztbe/furdHR0cvHTp8+XTMzMzU/P9/1cQDQL5Z9pby0tPSX2FZVbd68uauDAKAfLfuEOzw8\nXDMzM7Vly5a6+eab69y5c/XZZ5/V8PBwah8A9IVlg/vCCy/U8ePH69SpU3X27NkaGhqqsbGxGh8f\nT+0DgL6wbHAHBgZqfHxcYAHg/8nv4QJAgOACQIDgAkCA4AJAgOACQIDgAkCA4AJAgOACQIDgAkCA\n4AJAgOACQIDgAkCA4AJAgOACQIDgAkCA4AJAgOACQIDgAkCA4AJAgOACQIDgAkCA4AJAgOACQIDg\nAkCA4AJAgOACQIDgAkCA4AJAgOACQIDgAkCA4AJAgOACQMBAp9PpdOvkzWazW6cGgH+kVqt1xeON\nbl94cfRIty9BF4ycmap/tRZ7PYNVWmiOVOtD928tam4fqapy/9ao/96/K/FKGQACBBcAAgQXAAIE\nFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQX\nAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcA\nAgQXAAIEFwACBBcAAgQXAAIay304OTlZFy9e/MuxTqdTAwMDNT8/39VhANBPlg3uU089Vfv376+3\n3nqrBgcHU5sAoO8sG9zR0dGamJior7/+usbHx1ObAKDvLBvcqqqpqanEDgDoa35oCgACBBcAAgQX\nAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcA\nAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwAC\nBBcAAgQXAAIGOp1Op1snbzab3To1APwjtVqtKx5vdPvCi6NHun0JumDkzFTNLv671zNYpVdGdlXr\nw8Vez2AVmttHqqrcvzXqv/fvSrxSBoAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADB\nBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEF\ngADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgIAVB3dpaakbOwCgr101uCdOnKix\nsbEaHx+vjz/++PLxqampyDAA6CeNq31w+PDhOnr0aLXb7Zqenq4LFy7U9u3bq9PpJPcBQF+4anCv\nu+66uvXWW6uqam5urh5//PHasGFDDQwMxMYBQL+46ivlO++8sw4ePFjnz5+voaGhevPNN+vFF1+s\n7777LrkPAPrCVYN74MCBGh4evvxEu2HDhnrvvffqgQceiI0DgH5x1VfKjUajHnroob8cW79+fc3O\nznZ9FAD0G7+HCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGC\nCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYIL\nAAGCCwABggsAAYILAAGCCwABggsAAYILAAEDnU6n062TN5vNbp0aAP6RWq3WFY83un3hxdEj3b4E\nXTByZqpmF//d6xms0isju9y/NeqVkV1VVdVaXOzxElajOTJy1c+8UgaAAMEFgADBBYAAwQWAAMEF\ngADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWA\nAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYCA\nFQX3999/r6WlpW5tAYC+tWxwv/3223ryySdrZmamvvzyy9q2bVtt27atTp48mdoHAH2hsdyHzz//\nfE1PT9dPP/1Uu3fvrmPHjtX1119fU1NTNTY2ltoIAGvessFtt9t1zz33VFXVV199Vbfffvuf39RY\n9tsAgP9l2VfKGzdurNnZ2Wq323Xo0KGqqnrnnXdq/fr1kXEA0C+WfVR9+eWX68SJE7Vu3f90+Y47\n7qjJycmuDwOAfrJscNetW1dbt279y7GJiYmuDgKAfuT3cAEgQHABIEBwASBAcAEgQHABIEBwASBA\ncAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBw\nASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIGCg0+l0unXyZrPZ\nrVMDwD9Sq9W64vGuBhcA+JNXygAQILgAECC4ABAguAAQILgAECC4ABAguKvQbrfrueeeq4cffrgm\nJyfr+++/7/UkVujMmTM1OTnZ6xms0MWLF2vfvn31yCOP1I4dO+rTTz/t9ST+pj/++KNmZmZq586d\ntWvXrvrmm296PSlOcFfh+PHjtbS0VB988EHt3bu3Dh061OtJrMC7775b+/fvrwsXLvR6Civ00Ucf\n1W233Vbvv/9+HTlypF566aVeT+JvOnnyZFVVzc/P1549e+q1117r8aI8wV2FU6dO1ZYtW6qqavPm\nzbWwsNDjRazEXXfdVW+88UavZ7AK999/f01PT1dVVafTqcHBwR4v4u/aunXr5X8g/fzzz3XLLbf0\neFFeo9cD1qKzZ8/W0NDQ5a8HBwfr0qVL1Wj441wL7rvvvvrxxx97PYNVuOmmm6rqz7+Du3fvrj17\n9vR4ESvRaDTq6aefrk8++aRef/31Xs+J84S7CkNDQ3Xu3LnLX7fbbbGFkF9++aUee+yxmpiYqAcf\nfLDXc1ihV199tY4dO1bPPvtsnT9/vtdzogR3Fe6+++76/PPPq6rq9OnTtWnTph4vgmvDr7/+Wk88\n8UTt27evduzY0es5rMDRo0fr7bffrqqqG2+8sQYGBmrdumsrQR7LVmF8fLy++OKL2rlzZ3U6nTpw\n4ECvJ8E14fDhw/Xbb7/V3Nxczc3NVdWfPwR3ww039HgZ/5d77723ZmZm6tFHH61Lly7VM888c83d\nN/9bEAAEXFvP8wDQI4ILAAGCCwABggsAAYILAAGCCwABggsAAYILAAH/AaRupa33ZOK2AAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24862d72278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi_hat, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
