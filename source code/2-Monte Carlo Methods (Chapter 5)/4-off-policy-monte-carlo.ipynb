{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy Monte Carlo\n",
    "- Algorithms from ```pp. 84 - 92``` in Sutton & Barto 2017\n",
    "    - Off-policy prediction via importance sampling\n",
    "    - Off-policy MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "\n",
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate functions for generating policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_any_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        r = sorted(np.random.sample(3))\n",
    "        actions = env.actions\n",
    "        prob = [r[0], r[1] - r[0], r[2] - r[1], 1-r[2]]\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_greedy_policy(env, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        q_values = []\n",
    "        prob = []\n",
    "        \n",
    "        for a in env.actions:\n",
    "            actions.append(a)\n",
    "            q_values.append(Q[state,a])   \n",
    "        for i in range(len(q_values)):\n",
    "            if i == np.argmax(q_values):\n",
    "                prob.append(1)\n",
    "            else:\n",
    "                prob.append(0)       \n",
    "                \n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Action Values\n",
    "- Initialize all state-action values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.random()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_cum_sum(env):\n",
    "    c = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        c[(state, action)] = 0\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate episode\n",
    "- Generate episode based on current policy ($\\pi$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    episode = []\n",
    "    done = False\n",
    "    current_state = np.random.choice(env.states)\n",
    "    action = np.random.choice(policy[current_state][0], p = policy[current_state][1])\n",
    "    episode.append((current_state, action, -1))\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward = gw.state_transition(current_state, action)\n",
    "        action = np.random.choice(policy[current_state][0], p = policy[current_state][1])\n",
    "        episode.append((next_state, action, reward))\n",
    "        \n",
    "        if next_state == 0:\n",
    "            done = True\n",
    "        current_state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pi = generate_random_policy(gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-policy MC Prediction\n",
    "- Estimates $Q$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def off_policy_mc_prediction(env, pi, num_iter):\n",
    "    Q = state_action_value(env)\n",
    "    C = weight_cum_sum(env)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        b = generate_any_policy(env)\n",
    "        episode = generate_episode(gw, b)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for i in range(len(episode)-1, -1, -1):\n",
    "            s, a, r = episode[i]\n",
    "            if s != 0:\n",
    "                G = env.gamma * G + r\n",
    "                C[s,a] += W\n",
    "                Q[s,a] += (W / C[s,a]) * (G - Q[s,a])\n",
    "                W *= pi[s][1][pi[s][0].index(a)] / b[s][1][b[s][0].index(a)]\n",
    "                if W == 0:\n",
    "                    break\n",
    "                \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-policy MC Control\n",
    "- Finds optimal policy $pi \\approx pi_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def off_policy_mc_control(env, pi, num_iter):\n",
    "    Q = state_action_value(env)\n",
    "    C = weight_cum_sum(env)\n",
    "    pi = generate_greedy_policy(env, Q)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        b = generate_any_policy(env)\n",
    "        episode = generate_episode(gw, b)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for i in range(len(episode)-1, -1, -1):\n",
    "            s, a, r = episode[i]\n",
    "            if s != 0:\n",
    "                G = env.gamma * G + r\n",
    "                C[s,a] += W\n",
    "                Q[s,a] += (W / C[s,a]) * (G - Q[s,a])\n",
    "                pi = generate_greedy_policy(env, Q)\n",
    "                if a == pi[s][0][np.argmax(pi[s][1])]:\n",
    "                    break\n",
    "                W *= 1 / b[s][1][b[s][0].index(a)]\n",
    "\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q_hat, pi_hat = off_policy_mc_control(gw, generate_random_policy(gw), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (['U', 'D', 'L', 'R'], [0, 0, 1, 0]),\n",
       " 2: (['U', 'D', 'L', 'R'], [0, 1, 0, 0]),\n",
       " 3: (['U', 'D', 'L', 'R'], [0, 1, 0, 0]),\n",
       " 4: (['U', 'D', 'L', 'R'], [1, 0, 0, 0]),\n",
       " 5: (['U', 'D', 'L', 'R'], [1, 0, 0, 0]),\n",
       " 6: (['U', 'D', 'L', 'R'], [0, 0, 0, 1]),\n",
       " 7: (['U', 'D', 'L', 'R'], [0, 0, 0, 1]),\n",
       " 8: (['U', 'D', 'L', 'R'], [0, 0, 0, 1]),\n",
       " 9: (['U', 'D', 'L', 'R'], [0, 0, 1, 0]),\n",
       " 10: (['U', 'D', 'L', 'R'], [0, 0, 0, 1]),\n",
       " 11: (['U', 'D', 'L', 'R'], [0, 1, 0, 0]),\n",
       " 12: (['U', 'D', 'L', 'R'], [0, 0, 1, 0]),\n",
       " 13: (['U', 'D', 'L', 'R'], [0, 0, 1, 0]),\n",
       " 14: (['U', 'D', 'L', 'R'], [0, 0, 0, 1])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final policy obtained\n",
    "pi_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi_hat[s][0][np.argmax(pi_hat[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACsZJREFUeJzt3UtoXIXfxvFfmhEV4wUsSBGEbtoMCC0u3HURaLwUJFQK\ntkoUJCsXTaEWGVMveGnrSvAS6mXlQuPK4kIo1hYFBReFFgITRQTxtnElbbFpnfkv5N8XX9q8Ji/z\njJl+PrucQ8489DB8O4eEDHW73W4BAD21pt8DAOBqILgAECC4ABAguAAQILgAECC4ABDQ6OXFm81m\ntRcWevkS9EhzdLTubLt3q9V8c7TaH7l/q1Fz+2hVlfu3SjW3j1a73b7sOZ9wASBAcAEgQHABIEBw\nASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHAB\nIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEgQHABIEBwASBAcAEg\nQHABIEBwASDgHwe30+n0cgcADLTGUid//PHHOnjwYM3Pz1ej0ahOp1MbNmyoVqtV69evT20EgFVv\nyeDOzMzU3r17a9OmTZeOnTp1qlqtVs3NzfV8HAAMiiUfKS8uLv4ttlVVmzdv7ukgABhES37C3bhx\nY7VardqyZUvdeOONdfbs2fr8889r48aNqX0AMBCWDO7zzz9fx44dq5MnT9aZM2dqZGSkxsbGanx8\nPLUPAAbCksEdGhqq8fFxgQWA/ye/hwsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwAB\nggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGC\nCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAFD3W6326uLN5vNXl0aAP6V2u32ZY83\nev3CC5ve7fVL0AOjp6fcu1Vs9PRUzSx80O8ZrMDLo7uqqty/Veq/9+9yPFIGgADBBYAAwQWAAMEF\ngADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWA\nAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAA\nwQWAAMEFgADBBYAAwQWAgMZSJycnJ+vChQt/O9btdmtoaKjm5uZ6OgwABsmSwX3yySdr//799eab\nb9bw8HBqEwAMnCWDu2nTppqYmKhvvvmmxsfHU5sAYOAsGdyqqqmpqcQOABhofmgKAAIEFwACBBcA\nAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwAC\nBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIE\nFwACBBcAAoa63W63VxdvNpu9ujQA/Cu12+3LHm/0+oVnFj7o9UvQAy+P7qo72wv9nsEKzTdHvfdW\nqZdHd1VVVfsj77/VqLl99IrnPFIGgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEF\ngADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWA\nAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAAMEFgADBBYAAwQWAgGUHd3FxsRc7AGCgXTG4x48fr7Gx\nsRofH69PPvnk0vGpqanIMAAYJI0rnTh8+HAdOXKkOp1OTU9P1/nz52v79u3V7XaT+wBgIFwxuNdc\nc03dfPPNVVU1Oztbjz32WK1bt66GhoZi4wBgUFzxkfLtt99eBw8erHPnztXIyEi98cYb9cILL9T3\n33+f3AcAA+GKwT1w4EBt3Ljx0ifadevW1XvvvVf3339/bBwADIorPlJuNBr14IMP/u3Y2rVra2Zm\npuejAGDQ+D1cAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBc\nAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwA\nCBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBjqdrvdXl282Wz26tIA8K/Ubrcve7zR6xe+s73Q65eg\nB+abo+7dKjbfHK2ZhQ/6PYMVeHl0V1VVtRe8/1aj5ujoFc95pAwAAYILAAGCCwABggsAAYILAAGC\nCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYIL\nAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABggsAAYILAAGCCwABywru\nH3/8UYuLi73aAgADa8ngfvfdd/XEE09Uq9Wqr776qrZt21bbtm2rEydOpPYBwEBoLHXyueeeq+np\n6fr5559r9+7ddfTo0br22mtramqqxsbGUhsBYNVbMridTqfuvvvuqqr6+uuv69Zbb/3rmxpLfhsA\n8L8s+Uh5/fr1NTMzU51Opw4dOlRVVW+//XatXbs2Mg4ABsWSH1VfeumlOn78eK1Z8z9dvu2222py\ncrLnwwBgkCwZ3DVr1tTWrVv/dmxiYqKngwBgEPk9XAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAI\nEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQ\nXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACBjqdrvdXl282Wz26tIA\n8K/Ubrcve7ynwQUA/uKRMgAECC4ABAguAAQILgAECC4ABAguAAQI7gp0Op169tln66GHHqrJycn6\n4Ycf+j2JZTp9+nRNTk72ewbLdOHChdq3b189/PDDtWPHjvrss8/6PYl/6M8//6xWq1U7d+6sXbt2\n1bffftvvSXGCuwLHjh2rxcXF+vDDD2vv3r116NChfk9iGd55553av39/nT9/vt9TWKaPP/64brnl\nlnr//ffr3XffrRdffLHfk/iHTpw4UVVVc3NztWfPnnr11Vf7vChPcFfg5MmTtWXLlqqq2rx5c83P\nz/d5Ectxxx131Ouvv97vGazAfffdV9PT01VV1e12a3h4uM+L+Ke2bt166T9Iv/zyS9100019XpTX\n6PeA1ejMmTM1MjJy6evh4eG6ePFiNRr+OVeDe++9t3766ad+z2AFbrjhhqr66z24e/fu2rNnT58X\nsRyNRqOeeuqp+vTTT+u1117r95w4n3BXYGRkpM6ePXvp606nI7YQ8uuvv9ajjz5aExMT9cADD/R7\nDsv0yiuv1NGjR+uZZ56pc+fO9XtOlOCuwF133VVffPFFVVWdOnWqNmzY0OdFcHX47bff6vHHH699\n+/bVjh07+j2HZThy5Ei99dZbVVV1/fXX19DQUK1Zc3UlyMeyFRgfH68vv/yydu7cWd1utw4cONDv\nSXBVOHz4cP3+++81Oztbs7OzVfXXD8Fdd911fV7G/+Wee+6pVqtVjzzySF28eLGefvrpq+6++WtB\nABBwdX2eB4A+EVwACBBcAAgQXAAIEFwACBBcAAgQXAAIEFwACPgPqwelrdFfLd8AAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a657077320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi_hat, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
