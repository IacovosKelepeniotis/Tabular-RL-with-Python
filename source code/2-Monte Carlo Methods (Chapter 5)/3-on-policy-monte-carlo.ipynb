{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Monte Carlo\n",
    "- Algorithms from ```pp. 82 - 84``` in Sutton & Barto 2017\n",
    "- Estimates optimal policy $\\pi \\approx \\pi_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "\n",
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate policy\n",
    "- We start with equiprobable random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Action Values\n",
    "- Initialize all state-action values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = 0\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "q = state_action_value(gw)\n",
    "\n",
    "print(q[(2, \"U\")])\n",
    "print(q[(4, \"L\")])\n",
    "print(q[(10, \"R\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pi = generate_random_policy(gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate episode\n",
    "- Generate episode based on current policy ($\\pi$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    episode = []\n",
    "    done = False\n",
    "    current_state = np.random.choice(env.states)\n",
    "    action = np.random.choice(policy[current_state][0], p = policy[current_state][1])\n",
    "    episode.append((current_state, action, -1))\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward = gw.state_transition(current_state, action)\n",
    "        action = np.random.choice(policy[current_state][0], p = policy[current_state][1])\n",
    "        episode.append((next_state, action, reward))\n",
    "        \n",
    "        if next_state == 0:\n",
    "            done = True\n",
    "        current_state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 'U', -1),\n",
       " (2, 'D', -1),\n",
       " (6, 'R', -1),\n",
       " (7, 'U', -1),\n",
       " (3, 'D', -1),\n",
       " (7, 'D', -1),\n",
       " (11, 'U', -1),\n",
       " (7, 'U', -1),\n",
       " (3, 'L', -1),\n",
       " (2, 'D', -1),\n",
       " (6, 'D', -1),\n",
       " (10, 'D', -1),\n",
       " (14, 'R', -1),\n",
       " (0, 'R', -1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_episode(gw, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-policy Monte Carlo Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first-visit MC\n",
    "def on_policy_mc(env, pi, e, num_iter):\n",
    "    Q = state_action_value(env)\n",
    "    returns = dict()\n",
    "    for s, a in Q:\n",
    "        returns[(s,a)] = []\n",
    "        \n",
    "    for i in range(num_iter):\n",
    "        episode = generate_episode(env, pi)\n",
    "        already_visited = set({0})\n",
    "        for s, a, r in episode:\n",
    "            if s not in already_visited:\n",
    "                already_visited.add(s)\n",
    "                idx = episode.index((s, a, r))\n",
    "                G = 0\n",
    "                j = 1\n",
    "                while j + idx < len(episode):\n",
    "                    G = env.gamma * (G + episode[j + idx][-1])\n",
    "                    j += 1\n",
    "                returns[(s,a)].append(G)\n",
    "                Q[(s,a)] = np.mean(returns[(s,a)])\n",
    "        for s, _, _ in episode:\n",
    "            if s != 0:\n",
    "                actions = []\n",
    "                action_values = []\n",
    "                prob = []\n",
    "\n",
    "                for a in env.actions:\n",
    "                    actions.append(a)\n",
    "                    action_values.append(Q[s,a])         \n",
    "                for i in range(len(action_values)):\n",
    "                    if i == np.argmax(action_values):\n",
    "                        prob.append(1 - e + e/len(actions))\n",
    "                    else:\n",
    "                        prob.append(e/len(actions))        \n",
    "                pi[s] = (actions, prob)\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "# Obtained Estimates for Q & pi after 100000 iterations\n",
    "Q_hat, pi_hat = on_policy_mc(gw, generate_random_policy(gw), 0.2, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.8500000000000001, 0.05]),\n",
       " 2: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.8500000000000001, 0.05]),\n",
       " 3: (['U', 'D', 'L', 'R'], [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 4: (['U', 'D', 'L', 'R'], [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 5: (['U', 'D', 'L', 'R'], [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 6: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.8500000000000001, 0.05]),\n",
       " 7: (['U', 'D', 'L', 'R'], [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 8: (['U', 'D', 'L', 'R'], [0.8500000000000001, 0.05, 0.05, 0.05]),\n",
       " 9: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.05, 0.8500000000000001]),\n",
       " 10: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.05, 0.8500000000000001]),\n",
       " 11: (['U', 'D', 'L', 'R'], [0.05, 0.8500000000000001, 0.05, 0.05]),\n",
       " 12: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.05, 0.8500000000000001]),\n",
       " 13: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.05, 0.8500000000000001]),\n",
       " 14: (['U', 'D', 'L', 'R'], [0.05, 0.05, 0.05, 0.8500000000000001])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final policy obtained\n",
    "pi_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi_hat[s][0][np.argmax(pi_hat[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFJCAYAAAAxCJwFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22cbeab9080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi_hat, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
